{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983519e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "[INFO] processing image dataset/video_1\\00000.jpg 1/41\n",
      "[INFO] processing image dataset/video_1\\00001.jpg 2/41\n",
      "[INFO] processing image dataset/video_1\\00002.jpg 3/41\n",
      "[INFO] processing image dataset/video_1\\00003.jpg 4/41\n",
      "[INFO] processing image dataset/video_1\\00004.jpg 5/41\n",
      "[INFO] processing image dataset/video_1\\00005.jpg 6/41\n",
      "[INFO] processing image dataset/video_1\\00006.jpg 7/41\n",
      "[INFO] processing image dataset/video_1\\00007.jpg 8/41\n",
      "[INFO] processing image dataset/video_1\\00008.jpg 9/41\n",
      "[INFO] processing image dataset/video_1\\00009.jpg 10/41\n",
      "[INFO] processing image dataset/video_1\\00010.jpg 11/41\n",
      "[INFO] processing image dataset/video_1\\00011.jpg 12/41\n",
      "[INFO] processing image dataset/video_1\\00012.jpg 13/41\n",
      "[INFO] processing image dataset/video_1\\00013.jpg 14/41\n",
      "[INFO] processing image dataset/video_1\\00014.jpg 15/41\n",
      "[INFO] processing image dataset/video_1\\00015.jpg 16/41\n",
      "[INFO] processing image dataset/video_1\\00016.jpg 17/41\n",
      "[INFO] processing image dataset/video_1\\00017.jpg 18/41\n",
      "[INFO] processing image dataset/video_1\\00018.jpg 19/41\n",
      "[INFO] processing image dataset/video_1\\00019.jpg 20/41\n",
      "[INFO] processing image dataset/video_1\\00020.jpg 21/41\n",
      "[INFO] processing image dataset/video_1\\00021.jpg 22/41\n",
      "[INFO] processing image dataset/video_1\\00022.jpg 23/41\n",
      "[INFO] processing image dataset/video_1\\00023.jpg 24/41\n",
      "[INFO] processing image dataset/video_1\\00024.jpg 25/41\n",
      "[INFO] processing image dataset/video_1\\00025.jpg 26/41\n",
      "[INFO] processing image dataset/video_1\\00026.jpg 27/41\n",
      "[INFO] processing image dataset/video_1\\00027.jpg 28/41\n",
      "[INFO] processing image dataset/video_1\\00028.jpg 29/41\n",
      "[INFO] processing image dataset/video_1\\00029.jpg 30/41\n",
      "[INFO] processing image dataset/video_1\\00030.jpg 31/41\n",
      "[INFO] processing image dataset/video_1\\00031.jpg 32/41\n",
      "[INFO] processing image dataset/video_1\\00032.jpg 33/41\n",
      "[INFO] processing image dataset/video_1\\00033.jpg 34/41\n",
      "[INFO] processing image dataset/video_1\\00034.jpg 35/41\n",
      "[INFO] processing image dataset/video_1\\00035.jpg 36/41\n",
      "[INFO] processing image dataset/video_1\\00036.jpg 37/41\n",
      "[INFO] processing image dataset/video_1\\00037.jpg 38/41\n",
      "[INFO] processing image dataset/video_1\\00038.jpg 39/41\n",
      "[INFO] processing image dataset/video_1\\00039.jpg 40/41\n",
      "[INFO] processing image dataset/video_1\\00040.jpg 41/41\n",
      "Reading DATASET facenet_pytorch.pickle\n",
      "TEST DATASET include 41 items.\n",
      "(41, 512)\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image\n",
    "import imutils.paths as paths\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "mtcnn = MTCNN(device=device)\n",
    "\n",
    "# Create an inception resnet (in eval mode):\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# dataset = \"./FaceNet/dataset/Test/Dima/\"\n",
    "# dataset = \"./test7/\"\n",
    "dataset = \"dataset/\"\n",
    "pickle_file = \"facenet_pytorch.pickle\"\n",
    "\n",
    "imagepaths = list(paths.list_images(dataset))\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "for (i, imagePath) in enumerate(imagepaths):\n",
    "    print(\"[INFO] processing image {} {}/{}\".format(imagePath, i + 1, len(imagepaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    img = Image.open(imagePath)\n",
    "\n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "    # Get cropped and prewhitened image tensor\n",
    "    img_cropped = mtcnn(img)\n",
    "\n",
    "    # Calculate embedding (unsqueeze to add batch dimension)\n",
    "    encoding = resnet(img_cropped.unsqueeze(0)).detach().numpy()[0]\n",
    "\n",
    "    knownEncodings.append(encoding)  # векторизация\n",
    "    knownNames.append(name)\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    output = open(pickle_file, \"wb\")\n",
    "    pickle.dump(data, output)\n",
    "    output.close()\n",
    "\n",
    "test_encodings = pickle_file\n",
    "print('Reading DATASET', test_encodings)\n",
    "test_face_data = pickle.loads(open(test_encodings, \"rb\").read())\n",
    "count_data = test_face_data['names']\n",
    "print(f'TEST DATASET include {len(count_data)} items.')\n",
    "test_data = np.array(test_face_data['encodings'])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2672d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
